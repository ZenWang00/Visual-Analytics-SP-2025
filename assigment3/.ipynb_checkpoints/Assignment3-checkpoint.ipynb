{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Analytics\n",
    "\n",
    "## Assignment 3\n",
    "\n",
    "**Instructor:** Dr. Marco D'Ambros  \n",
    "**TAs:** Carmen Armenti, Mattia Giannaccari\n",
    "\n",
    "**Contacts:** marco.dambros@usi.ch, carmen.armenti@usi.ch, mattia.giannaccari@usi.ch\n",
    "\n",
    "**Due Date:** May 16, 2025 @ 23:55\n",
    "\n",
    "---\n",
    "The goal of this assignment is to use **Spark (PySpark)** and **Polars** in Jupyter notebooks.  \n",
    "The files `trip_data.csv`, `trip_fare.csv`, and `nyc_boroughs.geojson` are available in the provided folder: [Assignment3-data](https://usi365-my.sharepoint.com/:f:/g/personal/armenc_usi_ch/Ejp7sb8QAMROoWe0XUDcAkMBoqUFk-w2Vgroup025NhAww?e=2I7SMC).\n",
    "\n",
    "You may clean the data as needed; however, please note that specific data cleaning steps will be required in **Exercise 5**. If you choose to clean the data before Exercise 5, make sure to retain the **original dataset** for use with the Polars exercises.\n",
    "\n",
    "- Use **Spark** to solve **Exercises 1–4**\n",
    "- Use **Polars** to solve **Exercises 5–8**\n",
    "\n",
    "You are encouraged to use [Spark window functions](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html) whenever appropriate.\n",
    "\n",
    "Please name your notebook file as `SurnameName_Assignment3.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Join the `trip_data` and `trip_fare` dataframes into one and consider only data on 2013-01-01. Please specify the number of rows obtained after joining the 2 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAVA_HOME and PATH to use conda's OpenJDK 11 for PySpark compatibility\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/miniconda3/envs/jyvenv\"\n",
    "os.environ[\"PATH\"] = f\"/opt/miniconda3/envs/jyvenv/bin:\" + os.environ[\"PATH\"]\n",
    "# Now you can safely import and use PySpark in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/16 14:10:38 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/zitian/Visual-Analytics-SP-2025/assigment3\n",
      "Loading trip data...\n",
      "An error occurred: [PATH_NOT_FOUND] Path does not exist: file:/data/trip_data.csv.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/data/trip_data.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Load the data with error handling\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading trip data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m trip_data \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/trip_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m     inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPERMISSIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading trip fare data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m trip_fare \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/trip_fare.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m     inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPERMISSIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jyvenv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jyvenv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jyvenv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/data/trip_data.csv."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, count, when, isnan, isnull\n",
    "import os\n",
    "\n",
    "# Create and configure Spark session with more robust settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Data Analysis\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "    # Load the data with error handling\n",
    "    print(\"Loading trip data...\")\n",
    "    trip_data = spark.read.csv(\n",
    "        \"data/trip_data.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "        mode=\"PERMISSIVE\"\n",
    "    )\n",
    "\n",
    "    print(\"Loading trip fare data...\")\n",
    "    trip_fare = spark.read.csv(\n",
    "        \"data/trip_fare.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "        mode=\"PERMISSIVE\"\n",
    "    )\n",
    "\n",
    "    # Clean column names in trip_fare (remove leading spaces)\n",
    "    print(\"Cleaning column names...\")\n",
    "    trip_fare = trip_fare.withColumnRenamed(\" hack_license\", \"hack_license\") \\\n",
    "                        .withColumnRenamed(\" vendor_id\", \"vendor_id\") \\\n",
    "                        .withColumnRenamed(\" pickup_datetime\", \"pickup_datetime\") \\\n",
    "                        .withColumnRenamed(\" payment_type\", \"payment_type\") \\\n",
    "                        .withColumnRenamed(\" fare_amount\", \"fare_amount\") \\\n",
    "                        .withColumnRenamed(\" surcharge\", \"surcharge\") \\\n",
    "                        .withColumnRenamed(\" mta_tax\", \"mta_tax\") \\\n",
    "                        .withColumnRenamed(\" tip_amount\", \"tip_amount\") \\\n",
    "                        .withColumnRenamed(\" tolls_amount\", \"tolls_amount\") \\\n",
    "                        .withColumnRenamed(\" total_amount\", \"total_amount\")\n",
    "\n",
    "    # Display schema of the dataframes\n",
    "    print(\"\\nTrip Data Schema:\")\n",
    "    trip_data.printSchema()\n",
    "\n",
    "    print(\"\\nTrip Fare Schema:\")\n",
    "    trip_fare.printSchema()\n",
    "\n",
    "    # Convert pickup_datetime to date type and cache for better performance\n",
    "    print(\"\\nConverting dates...\")\n",
    "    trip_data = trip_data.withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\"))).cache()\n",
    "    trip_fare = trip_fare.withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\"))).cache()\n",
    "\n",
    "    # Filter data for January 1, 2013\n",
    "    print(\"\\nFiltering data for 2013-01-01...\")\n",
    "    trip_data_jan1 = trip_data.filter(col(\"pickup_date\") == \"2013-01-01\")\n",
    "    trip_fare_jan1 = trip_fare.filter(col(\"pickup_date\") == \"2013-01-01\")\n",
    "\n",
    "    # Join the two dataframes on common keys\n",
    "    print(\"\\nJoining datasets...\")\n",
    "    joined_data = trip_data_jan1.join(\n",
    "        trip_fare_jan1,\n",
    "        on=[\"medallion\", \"hack_license\", \"vendor_id\", \"pickup_datetime\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Count the number of rows after joining\n",
    "    print(\"\\nCounting rows...\")\n",
    "    row_count = joined_data.count()\n",
    "    print(f\"Number of rows after joining the datasets for 2013-01-01: {row_count}\")\n",
    "\n",
    "    # Show a sample of the joined data\n",
    "    print(\"\\nSample of joined data:\")\n",
    "    joined_data.show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Clean up cached data\n",
    "    if 'trip_data' in locals():\n",
    "        trip_data.unpersist()\n",
    "    if 'trip_fare' in locals():\n",
    "        trip_fare.unpersist()\n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Provide a graphical representation to compare the average fare amount for trips _within_ and _across_ all the boroughs. You may want to have a look at: https://docs.bokeh.org/en/latest/docs/user_guide/topics/categorical.html#categorical-heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Consider only Manhattan, Bronx and Brooklyn boroughs. Then create a dataframe that shows the total number of trips *within* the same borough and *across* all the other boroughs mentioned before (Manhattan, Bronx, and Brooklyn) where the passengers are more or equal than 3.\n",
    "\n",
    "For example, for Manhattan borough you should consider the total number of the following trips:\n",
    "- Manhattan → Manhattan\n",
    "- Manhattan → Bronx\n",
    "- Manhattan → Brooklyn\n",
    "\n",
    "You should then do the same for Bronx and Brooklyn boroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Create a dataframe where each row represents a driver, and there is one column per borough.\n",
    "For each driver-borough, the dataframe provides the maximum number of consecutive trips\n",
    "for the given driver, within the given borough. Please consider only trips which were payed by card. \n",
    "\n",
    "For example, if for driver A we have (sorted by time):\n",
    "- Trip 1: Bronx → Bronx\n",
    "- Trip 2: Bronx → Bronx\n",
    "- Trip 3: Bronx → Manhattan\n",
    "- Trip 4: Manhattan → Bronx.\n",
    "    \n",
    "The maximum number of consecutive trips for Bronx is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Please work on the merged dataset of trips and fares and perform the following data cleaning tasks:\n",
    "\n",
    "1. Remove trips with invalid locations (i.e. not in New York City);\n",
    "3. Remove trips with invalid amounts:\n",
    "    - Total amount must be greater than zero;\n",
    "    - Total amount must correspond to the sum of all the other amounts.\n",
    "5. Remove trips with invalid time:\n",
    "    - Pick-up before drop-off;\n",
    "    - Valid duration.\n",
    "\n",
    "After each data cleaning task, report how many rows where removed. Finally report:\n",
    "- Are there **duplicate trips**?\n",
    "- How many trips remain after cleaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Compute the **total revenue** (total_amount) grouped by:\n",
    "- Pick-up hour of the day (0–23)\n",
    "- Passenger count (group >=6 into “6+”)\n",
    "\n",
    "Create a heatmap where:\n",
    "- X-axis = hour\n",
    "- Y-axis = passenger count group\n",
    "- Cell value = average revenue per trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Define an \"anomalous trip\" as one that satisfies at least two of the following:\n",
    "- Fare per mile is above the 95th percentile\n",
    "- Tip amount > 100% of fare\n",
    "- trip_time_in_secs is less than 60 seconds but distance is more than 1 mile\n",
    "\n",
    "Create a dataframe of anomalous trips and:\n",
    "- Report how many such trips exist\n",
    "- Create a scatterplot to visualize the anomaly metrics\n",
    "- Describe the visualization identifying groups and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "For each driver (hack_license), calculate the **total profit per hour worked**, where:\n",
    "> profit = 0.7 * (fare_amount + tip_amount) when the trip starts between 7:01 AM and 7:00 PM\\\n",
    "> profit = 0.8 * (fare_amount + tip_amount) when the trip starts between 7:01PM and 7:00 AM\n",
    "\n",
    "Estimate \"hours worked\" by summing trip_time_in_secs.\n",
    "\n",
    "Plot a line chart showing the distribution of average profit per hour **for the top 10% drivers** in terms of total trips.\n",
    "\n",
    "Which time of day offers **best earning efficiency**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jyvenv)",
   "language": "python",
   "name": "jyvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
